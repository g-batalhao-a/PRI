{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries\n",
    "\n",
    "- \"Apple pie\"\n",
    "- \"Chicken\" in the African category\n",
    "- \"Easy bread\" less than 2h\n",
    "- \"Pasta bolognese\"\n",
    "- \"Oatmeal\"\n",
    "\n",
    "Prioritize doing 3 of the queries first and running test models, then add more if possible.\n",
    "\n",
    "# Scoring\n",
    "\n",
    "Human classification of top 100 results obtained using the standard system (LTR-less). Scoring is done on a numeric scale from 0-5.\n",
    "\n",
    "## Criteria\n",
    "\n",
    "The attribution of a given score is a bit subjective but tries to follow the following guidelines:\n",
    "\n",
    "0. A document that does not match the query.\n",
    "1. A document that vaguely matches the query, is very incomplete (missing important fields, like instructions) and has no reviews. Or has very negative reviews.\n",
    "2. A document that partially matches the query, is incomplete and has no reviews. Or a document with negative reviews.\n",
    "3. A document that matches the query semantically, is reasonably complete (may miss more than two fields) and has at least one positive review.\n",
    "4. A document that perfectly or almost perfectly matches the query semantically, is complete or missing just one of the fields and has a good number of positive reviews (5 to 20).\n",
    "5. A document that perfectly matches the query semantically, is complete (the recipe has a full ingredient list, steps and cook time/nutritional information) and has a lot of positive reviews (more than 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as urlp\n",
    "\n",
    "URL = \"http://localhost:8983/solr/recipes/select\"\n",
    "URL += \"?rows=100\"\n",
    "URL += \"&q.op=AND\"\n",
    "URL += \"&q={q}\"\n",
    "URL += \"&qf=\" + \"Name^5 Description Ingredients^2 Keywords^2 Instructions Reviews^0.5 AuthorName^0.2\"\n",
    "URL += \"&wt=json\"\n",
    "URL += \"&defType=edismax\"\n",
    "URL += \"&fl=id,RecipeId,score,[features]\"\n",
    "URL += \"&rq={{!ltr model=myModel reRankDocs=100 efi.text={q}}}\"\n",
    "URL += \"&fq={fq}\"\n",
    "\n",
    "query = [\"apple pie\", \"chicken\", \"easy bread\", \"pasta bolognese\", \"oatmeal\"]\n",
    "facet = [\"\", \"Category_Facet:African\", \"\", \"\", \"\"]\n",
    "urls = [URL.format(q=query[i], fq=facet[i]) for i in range(len(query))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import simplejson\n",
    "import pandas as pd\n",
    "\n",
    "for (idx, url) in enumerate(urls):\n",
    "    response = requests.request(\"GET\", url)\n",
    "    json = simplejson.loads(response.text)\n",
    "\n",
    "    for doc in json[\"response\"][\"docs\"]:\n",
    "        doc[\"URL\"] = \"http://localhost:3000/recipe/{0}\".format(doc[\"RecipeId\"]) \n",
    "        doc[\"query\"] = query[idx]\n",
    "        doc[\"facet\"] = facet[idx]\n",
    "    \n",
    "    df = pd.DataFrame(json[\"response\"][\"docs\"])\n",
    "    df.to_csv(\"queries/query{0}_results.csv\".format(idx+1), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Solr's LTR implementation supports two different kinds of models: Linear and Tree Based.\n",
    "\n",
    "There are various algorithms that may be used in order to create these models. We will demonstrate the use of two, one for each type:\n",
    "\n",
    "- A Linear Model using Support Vector Machines\n",
    "- A Neural Network model built using RankNet\n",
    "\n",
    "We will use SciKit Learn's SVM implementation and a RankNet implementation built into Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import simplejson\n",
    "import glob\n",
    "\n",
    "result_files = glob.glob(\"queries/*_results.csv\")\n",
    "scores_files = glob.glob(\"queries/*_scores.csv\")\n",
    "\n",
    "result_files.sort()\n",
    "scores_files.sort()\n",
    "\n",
    "inputs = pd.concat((pd.read_csv(file) for file in result_files), ignore_index=True)\n",
    "scores = pd.concat((pd.read_csv(file) for file in scores_files), ignore_index=True)\n",
    "\n",
    "X = []\n",
    "Y = [entry.score for entry in scores.itertuples()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(entry):\n",
    "    req_url = \"http://localhost:8983/solr/recipes/select?rows=100&q.op=AND&q={q}&qf=Name^5%20Description%20Ingredients^2%20Keywords^2%20Instructions%20Reviews^0.5%20AuthorName^0.2&wt=json&defType=edismax&fl=[features]&fq={fq}&rq={rq}\"\n",
    "    facet = entry.facet\n",
    "    if pd.isna(facet):\n",
    "        facet = \"\"\n",
    "    response = requests.request(\"GET\", req_url.format(q=entry.query, fq=f\"RecipeId:{entry.RecipeId} {facet}\", rq=f\"{{!ltr model=myModel efi.text='{entry.query}'}}\"))\n",
    "    json = simplejson.loads(response.text)\n",
    "    return [float(feature.split(\"=\")[1]) for feature in json[\"response\"][\"docs\"][0][\"[features]\"].split(\",\")]\n",
    "\n",
    "\n",
    "for entry in inputs.itertuples():\n",
    "    X.append(get_features(entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "(train_x,\n",
    " test_x,\n",
    " train_y,\n",
    " test_y) = train_test_split(X, Y, test_size=0.25, random_state=1, stratify=Y)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/.asdf/installs/python/anaconda3-2020.11/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02679156372444591"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "linearSVM = svm.LinearSVR()\n",
    "lienarReg = linear_model.LinearRegression()\n",
    "\n",
    "linearSVM.fit(train_x, train_y)\n",
    "lienarReg.fit(train_x, train_y)\n",
    "\n",
    "pred_svm = linearSVM.predict(test_x)\n",
    "pred_reg = lienarReg.predict(test_x)\n",
    "\n",
    "r2_score(test_y, pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_file.dat\", 'w') as file:\n",
    "    for i in range(len(result_files)):\n",
    "        in_f = pd.read_csv(result_files[i])\n",
    "        s_f = pd.read_csv(scores_files[i])\n",
    "        feats = [\" \".join([f\"{idx+1}:{f}\" for idx, f in enumerate(get_features(entry))]) for entry in in_f.itertuples()]\n",
    "        r_ids = [entry.RecipeId for entry in in_f.itertuples()]\n",
    "        scores = [entry.score for entry in s_f.itertuples()]\n",
    "        file.writelines(f\"{s} qid:{i} {f} # {id}\\n\" for f, s, id in zip(feats, scores, r_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, url) in enumerate(urls):\n",
    "    response = requests.request(\"GET\", url)\n",
    "    json = simplejson.loads(response.text)\n",
    "\n",
    "    scores = pd.read_csv(scores_files[i])\n",
    "\n",
    "    for doc in json[\"response\"][\"docs\"]:\n",
    "        doc[\"score\"] = scores.loc[scores[\"RecipeId\"] == doc[\"RecipeId\"]][\"score\"].values\n",
    "        if len(doc[\"score\"]) > 0:\n",
    "            doc[\"score\"] = doc[\"score\"][0]\n",
    "        else:\n",
    "            doc[\"score\"] = \"\"\n",
    "        \n",
    "    df = pd.DataFrame(json[\"response\"][\"docs\"])\n",
    "    df.to_csv(\"queries/query{0}_ltr.csv\".format(i+1), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.365997314453125\n"
     ]
    }
   ],
   "source": [
    "def probability_satisfied(grade):\n",
    "    return (pow(2, grade) - 1) / 32\n",
    "\n",
    "def probability_not_satisfied(probs):\n",
    "    pns = 1\n",
    "    for p in probs:\n",
    "        pns = pns * (1 - p)\n",
    "    return pns\n",
    "\n",
    "def err(grades):\n",
    "    err = 0\n",
    "    probs = []\n",
    "\n",
    "    for i, grade in enumerate(grades):\n",
    "        k = i + 1\n",
    "        ps = probability_satisfied(grade)\n",
    "        pns = probability_not_satisfied(probs)\n",
    "        \n",
    "        err = err + (1 / k) * ps * pns\n",
    "\n",
    "        probs.append(ps)\n",
    "\n",
    "    return err\n",
    "\n",
    "print(err([3, 2, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== QUERY 1 ====\n",
      "Original: [4, 2, 3, 3, 2, 2, 2, 5, 3, 3, 3, 2, 3, 3, 3]\n",
      "Original score: 0.5892181081973864\n",
      "Ltr: [3.0, 3.0, 4.0, 3.0, 3.0, 3.0, 5.0, 3.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 5.0]\n",
      "Ltr score: 0.457488682640339\n",
      "\n",
      "==== QUERY 2 ====\n",
      "Original: [4, 4, 2, 3, 3, 5, 4, 4, 4, 3, 4, 2, 2, 2, 2]\n",
      "Original score: 0.6506136785874781\n",
      "Ltr: [4, 5, 4, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0]\n",
      "Ltr score: 0.7290675037691942\n",
      "\n",
      "==== QUERY 3 ====\n",
      "Original: [2, 4, 5, 3, 1, 5, 4, 1, 4, 4, 2, 4, 3, 2, 4]\n",
      "Original score: 0.46439582030465915\n",
      "Ltr: [3.0, 5.0, 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 5.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0]\n",
      "Ltr score: 0.6035058210613998\n",
      "\n",
      "==== QUERY 4 ====\n",
      "Original: [3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2]\n",
      "Original score: 0.364180810561534\n",
      "Ltr: [3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3]\n",
      "Ltr score: 0.4076726652419933\n",
      "\n",
      "==== QUERY 5 ====\n",
      "Original: [4, 2, 3, 3, 1, 4, 4, 3, 3, 4, 3, 4, 4, 4, 4]\n",
      "Original score: 0.5915420940740799\n",
      "Ltr: [4.0, 5.0, 5.0, 3.0, 3.0, 5.0, 5.0, 4.0, 4.0, 4.0, 5.0, 4.0, 4.0, 3.0, 4.0]\n",
      "Ltr score: 0.7315337742844144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ltr_files = glob.glob(\"queries/*_ltr.csv\")\n",
    "ltr_files.sort()\n",
    "\n",
    "for (i, ltr_file) in enumerate(ltr_files):\n",
    "    og_scores = [entry.score for entry in pd.read_csv(scores_files[i]).itertuples()][:15]\n",
    "    ltr_scores = [entry.score for entry in pd.read_csv(ltr_file).itertuples()][:15]\n",
    "    print(f\"==== QUERY {i+1} ====\")\n",
    "    print(f\"Original: {og_scores}\")\n",
    "    print(f\"Original score: {err(og_scores)}\")\n",
    "    print(f\"Ltr: {ltr_scores}\")\n",
    "    print(f\"Ltr score: {err(ltr_scores)}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a6c239678afad6960c456aefc8713a9d49e3f9a7a9bb744edb151330f0438dc"
  },
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
